# ğŸš€ H-002 Customer Experience Automation

**Tagline:** A hyper-personalized retail assistant that transforms vague customer messages into location-aware, preference-aware, context-rich actions â€” powered by RAG, real-time data, and privacy-safe AI.

---

# 1. The Problem (Real World Scenario)

### **Context**

Modern retail brands receive thousands of customer messages every day:

* â€œIâ€™m cold.â€
* â€œWhat can I get quickly?â€
* â€œWhereâ€™s the nearest store with coffee?â€
* â€œDo I have any coupons left?â€
* â€œWhere is my order?â€

Standard chatbots respond with:

> *â€œCan you clarify your issue?â€*

But customers don't want clarification â€” they want **solutions**, immediately.

### **The Pain Point**

Retail brands suffer from:

* **Inefficient support agents** asking repetitive questions (â€œWhatâ€™s your email?â€, â€œWhere are you?â€)
* **Lost sales** because chatbots fail to convert vague intent into purchases
* **Poor experience** due to irrelevant recommendations
* **Slow response loops** (especially for location-based or offer-based queries)

### **My Solution**

I built **H-002 Customer Experience Automation**, a full-stack, production-minded system that:

ğŸ’¬ Interprets vague user messages
ğŸ“ Combines them with real-time location
ğŸ§¾ Reads internal policy & FAQs using RAG
ğŸ›ï¸ Checks user purchase history
ğŸŸï¸ Fetches personalized coupons
ğŸ”’ Masks all PII before LLM usage
âš¡ Returns a precise, contextual, actionable response

Example:
**User:** *â€œIâ€™m cold.â€*
**AI:**

> â€œYouâ€™re 43m from Downtown CafÃ©. They have Hot Chocolate at 15% off. Want me to apply the coupon?â€

This is not a chatbot â€”
Itâ€™s an **intelligent micro-journey engine** tailored for retail.

---

# 2. Expected End Result (User Experience)

### **Input**

User logs into the Streamlit app, opens chat, and types a vague natural-language message like:

* â€œIâ€™m hungryâ€
* â€œWhere should I go?â€
* â€œAny offers?â€

### **Action**

The system automatically:

1. Fetches the user profile from MongoDB
2. Loads order history + preferences
3. Retrieves nearby stores based on a simulated location
4. Pulls available coupons
5. Runs RAG retrieval from policy & FAQ documents
6. Masks all PII
7. Sends a compact structured prompt to the LLM
8. Generates a short, actionable answer

### **Output**

A rich, actionable message that may include:

* Nearest store and walking distance
* Personalized coupon
* Recommended items based on preferences
* Relevant policy snippet (refund, privacy, order tracking)
* Upsell opportunities (â€œWould you like a warm drink?â€)

**All inside a modern Streamlit UI with history, personalization, and API usage tracking.**

---

# 3. Technical Approach (Production-Ready Design)

I wanted this project to go beyond a simple chatbot.
I built a **modular, realistic, production-aligned system** with privacy, rate-limiting, retrieval, and orchestration.

---

## âœ” **Pipeline Overview**

### **1. Data Layer (MongoDB)**

The database stores:

* Users
* Orders
* Stores
* Coupons
* Embeddings
* Chat history
* Daily LLM usage counters

### **2. PII Masking Layer**

Before sending **any text** to LLM:

* Emails â†’ `<EMAIL_1>`
* Phone numbers â†’ `<PHONE_1>`
* Card numbers â†’ `<CARD_1>`
* Addresses â†’ `<ADDRESS_1>`

Supports **deterministic masking** so user experience remains consistent.

### **3. Retrieval-Augmented Generation (RAG)**

Internal policies & FAQs are chunked â†’ embedded â†’ stored â†’ retrieved in real time.

Prevents hallucinations and ensures compliance.

### **4. Context Enrichment**

We build a structured prompt using:

* Masked user query
* Nearest store (Haversine distance)
* Coupons
* Preferences
* Order history summary
* RAG snippets

### **5. LLM Layer**

A lightweight model (gpt-3.5 / gpt-4.1-mini) generates:

* Short
* Actionable
* Personalized
* Safe responses

A smart retry + limit-aware client ensures:

* No socket explosion
* No infinite retries
* No excessive token usage

### **6. UI Layer (Streamlit)**

Shows:

* Chat bubbles
* Sidebar with profile
* API usage bar
* Debug toggle for retrieved context

---

## âœ” **System Architecture**

```
User â†’ Streamlit UI â†’ Auth â†’ Chat Engine â†’ PII Masker â†’ RAG â†’ Context Builder â†’ LLM Client â†’ Response
```

Modules communicate cleanly using:

* `db.py` for all database IO
* `llm_client.py` for all model calls
* `rag_pipeline.py` for document retrieval
* `rate_limiter.py` for quota governance
* `utils.py` for summarization + geolocation

A pipeline built to be **extendable, auditable, and reliable**.

---

# 4. Tech Stack

### **Language**

* Python 3.10

### **Backend**

* Streamlit
* Python (modular architecture)
* Pydantic

### **Database**

* MongoDB (PyMongo)

### **Retrieval**

* Sentence Transformers (embeddings)
* Simple vector similarity (MongoDB stored norms)

### **AI Model**

* OpenAI GPT-3.5 / GPT-4.1-mini
* (Swappable: Groq, Gemini, OpenRouter)

### **Security**

* BCrypt hashed passwords
* PII masking before LLM
* Daily per-user rate limits

### **UX**

* Custom CSS
* Chat UI
* Sidebar with live usage stats

### **Utilities**

* Faker
* Pypdf
* Haversine formula

---

# 5. Challenges & Learnings

### **Challenge 1 â€” PII Exposure Risk**

Problem:
LLMs should *never* see raw user phone numbers, emails, or order IDs.

Solution:
A deterministic PII masking module + system prompt guard that forbids reconstruction.

---

### **Challenge 2 â€” API Rate Limits & Network Stability**

Problem:
Windows aggressively throttles socket creation â†’ hitting OpenAI limits caused:

* `ConnectionResetError 10054`
* `WinError 10055`

Solution:

* Added **bounded retry logic**
* Prevented recursive retry loops
* Added local mocking mode (`MOCK_LLM`)
* Added per-user daily quota

---

### **Challenge 3 â€” RAG Relevance**

Initially, RAG fetched noisy chunks.

Solution:

* Tighter chunk sizes
* Improved embedding metadata
* Retrieved only top-3 snippets for token efficiency

---

### **Challenge 4 â€” Generating Personalized Responses**

Not just â€œWhat do you want?â€
But:

* Considering user history
* Considering location
* Considering store offers

Solution:
A structured system prompt that guides the LLM to behave like:

**â€œA hyper-personalized retail concierge.â€**

---

# 6. Visual Proof (What Reviewers Will See)

### âœ” Login & Signup Page

Beautiful Streamlit UI with validation and secure password hashing.

### âœ” Chat Interface

Human-like bubbles with:

* User messages (right aligned)
* AI responses (left aligned)

### âœ” Sidebar

Displays:

* User profile
* Preferences
* API usage bar
* Retrieved RAG snippets

### âœ” Database Snapshots

MongoDB collection for:

* users
* coupons
* stores
* orders
* chat_history

---

# 7. How to Run

```bash
# 1. Clone Repository
git clone https://github.com/username/H002-Customer-Experience-Automation.git
cd H002-Customer-Experience-Automation

# 2. Create venv
python -m venv venv
.\venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Add .env
# (Mongo + OpenAI keys)
notepad .env

# 5. Generate synthetic dataset
python -m h002_customer_experience_automation.data_generator

# 6. Run the app
streamlit run h002_customer_experience_automation/app.py
```

---

# 8. Project Structure

```
h002_customer_experience_automation/
â”‚â”€â”€ app.py
â”‚â”€â”€ auth.py
â”‚â”€â”€ config.py
â”‚â”€â”€ db.py
â”‚â”€â”€ llm_client.py
â”‚â”€â”€ pii_masker.py
â”‚â”€â”€ rag_pipeline.py
â”‚â”€â”€ rate_limiter.py
â”‚â”€â”€ data_generator.py
â”‚â”€â”€ utils.py
â””â”€â”€ README.md
```

